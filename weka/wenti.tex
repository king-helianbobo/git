\section{实施方案}
\begin{enumerate}[(1)]
\item 繁简转换，（问题不难，使用繁简转换表）。
\item 拼音转汉字，（属于优化的问题，现在没必要做）。
\item 同音词拼写错误，先把汉字转成拼音，然后进行可能的比对，比如中华人名共和国-->中华人民共和国，这个已经实现。
\item 英文拼写错误，（暂时不作考虑，可以考虑使用开源的库）。
\item 形近词错误，（这个需要吗，当用户使用五笔或记错某个字的样子时）
\end{enumerate}
\subsection{网站群的一些问题}
\par 政府网站中的特殊字符需要进行转换。
\par 当对政府网站中的网页建立索引时，如果使用url作为文档id，相同内容的文档可能出现多次。下面两个url仅仅是字符大小写不同，却作为不同的id出现。解决办法是文档id一律小写。
\begin{verbatim}
http://www.wuxi.gov.cn/WebPortal/AskAnswer/Gov_AskAnswer_Info?
AnswerID=9174e3c9-4bb6-41cd-ba75-6465a3aa490c
http://www.wuxi.gov.cn/WEBPORTAL/AskAnswer/Gov_AskAnswer_Info?
AnswerID=9174e3c9-4bb6-41cd-ba75-6465a3aa490c
\end{verbatim}
\par 下面两个url均包含了“$\backslash$”字符，其后跟随普通ASCII字符时，转化成一种转义字符，作为文档id会出错。
\begin{verbatim}
http://www.wuxi.gov.cn/WEBPORTAL/ChiefHall/ChiefHallInfoDetailsXK?SystemID
=4028818a28aff36d0128b3861a090c76&ChiefHallType=xk360ChromeURL\Shell\Open\Command
http://www.wuxi.gov.cn/WEBPORTAL/ChiefHall/ChiefHallInfoDetailsXK?SystemID
=4028818a2fbfee4b012fc2fe529909ac&ChiefHallType=xk360ChromeURL\Shell\Open\Command
\end{verbatim}
\par 爬虫获得的网页内容中，如标题为《京杭大运河无锡段》，而内容为《京杭大运河无锡段 发布时间： 2011年11月23日 修改时间： 2013年10月22日 [ 大 中 小 ] 浏览次数：》，这部分内容重复，影响了检索的相关性，需将其过滤。
\par “春涛”这个词，结合上下文，分词结果不同，下面两个例子，前者分成了“春，涛”，后者分成了“春涛”，还有词，比如：“易视腾”，“买卖宝”。
\begin{verbatim}
curl -XGET 'namenode:9200/official_mini/_analyze?analyzer=soul_index&pretty' -d '登太湖仙岛、观鼋渚春涛'
curl -XGET 'namenode:9200/official_mini/_analyze?analyzer=soul_index&pretty' -d '鼓浪屿上听春涛'
\end{verbatim}
\par 对2014年3月18号爬取的网站群数据，作分词处理后，共有164976个不同的词，总共的词个数是161630251个，使用word2vec代码如下，最后一行代码对SogouR.txt进行编码转换。
\begin{verbatim}
./word2vec -train /mnt/f/b.txt -output vectors.bin -cbow 0 -size 200 -window 7 -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 1
./distance vectors.bin
cat SogouR.txt | iconv -f gbk -t utf8 -c > SogouR-utf8.txt
\end{verbatim}
\par synonym-new.txt中与synonym.txt没有交集的词条共5689个，与其有包含或被包含关系的词条共1464个（有效条目是1318个）,剩下的都是有交集的，扩展后的词条为12715个。synonym.txt中与synonym-new.txt没有交集的词条共1893个，最终的词条应该是21613个。
\par 在ShardSuggestService中，有若干个隶属于IndexShard的cache，目前保存的cache主要有：用于拼写检查的spellCheckCache，用于智能提示的titleSuggestCache，用于统计每个term\footnote{Lucene术语，代表一个Token。}的document frequency（term的文档数目）和total frequency（term总共出现次数）的termListCache。使用如下命令，获取term“垃圾”的两个频率，当结合词性识别关键字时，该命令非常有用（命令中的size暂时没用）。
\begin{verbatim}
curl -XGET 'localhost:9200/official_mini/_termlist?pretty' -d '{
    "action": "termlist", 
    "fields": [
        "content", 
        "contenttitle"
    ], 
    "size": 0, 
    "term": "垃圾"
}'
\end{verbatim}
\par 使用如下命令，获取“驾驶证补办”的标题提示。
\begin{verbatim}
curl -X POST localhost:9200/official_mini/table/__suggest?pretty -d '{
    "field": "contenttitle", 
    "size": 15, 
    "term": "驾驶证补办", 
    "type": "synonym"
}'
\end{verbatim}
\par 第一个命令，装载标题域《contenttitle》的cache，如果cache存在，则直接返回，否则装载之。第二个命令，刷新标题域《contenttitle》的cache，如果cache不存在，则返回，否则刷新该cache。type为suggest表示装载titleSuggestCache，如果为spell，表示装载spellCheckCache。
\begin{verbatim}
curl -XPOST localhost:9200/official_mini/table/__suggestRefresh -d'{
    "action": "load", 
    "field": "contenttitle", 
    "type": "suggest"
}'
curl -XPOST localhost:9200/official_mini/table/__suggestRefresh -d'{   
    "action": "refresh", 
    "field": "contenttitle", 
    "type": "suggest"
}'
\end{verbatim}
\par 使用curl命令搜索第7页（注意pn=60，百度默认一个页面10条记录）的“成奎安”，
\begin{verbatim}
http://www.baidu.com/s?wd=成奎安&pn=60
先将cookies存入文件cookie.txt（位于当前目录）
curl -c cookie.txt www.baidu.com
cat cookie.txt
当前url使用指定的cookies文件，-A后的字符串为user agent，该agent拷贝自FireFox。
curl -v --cookie ./cookie.txt  -A "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1)"  "http://www.baidu.com/s?^&wd=成奎安&pn=0";
《url为http://www.baidu.com/s?cl=3^&wd=×××》貌似也可。
如被百度防御系统封闭，考虑删除cookies文件，再重新生成cookies文件。
\end{verbatim}
\subsection{同步网页文件}
\par 使用nutch时，如果目录中文件太多，且包含了很多不需要的类型（如音频，视频），可使用rsync命令，保持源文件夹目录结构，同时只提取指定类型。
\begin{verbatim}
rsync -av --exclude='path1/exclude' --exclude='path2/exclude' [source] [dest]
rsync -av --include='*.shtml' --include='*.doc' --include='*.pdf' --include='*/' --exclude='*' /mnt/d data/
rsync -av --exclude='wap2013' --exclude='wap' --exclude='blind' --exclude='blind1111111111'  --include='*.shtml' --include='*/' --exclude='*' /mnt/d /opt/data-07-23/
\end{verbatim}
\par '*/'表示提取目录，'*'表示忽略文件类型，注意[source]与[source/]不同，前者表示拷贝source目录，后者表示拷贝目录下的内容。如果需要过滤的文件太多，可使用[--exclude-from=FILE]参数，其中FILE存储过滤的文件或目录，也可过滤类型，如[--exclude=*/.svn*]。
\par 将远程镜像目录/mnt/d的内容同步到本地目录/opt/data-07-23/后，再用程序读取data-07-23目录下有哪些新增网页（根据html文件的生成时间判断），然后用nutch解析这些网页，nutch解析后的网页发送给ES即可。
\subsection{数据导入ES过程}
\par 首先在ES中建立每个域的类型和分词方式(mapping)，政府网站搜索有：url域（不需要分词），标题域（contenttitle），正文域（content），时间域（postTime，不需要分词），关键词（对应信息公开目录）以及网页分类（魅力锡城，锡城资讯，信息公开，政务大厅，政民互动等，不需要分词）。
\par 每个网页，经数据处理后转换成一个Map对象，这些Map对象以bulk方式写入ES，根据上一步中的mapping，ES会自动为每个域建立索引，负载均衡，索引分片，副本拷贝等操作均由ES负责。nutch处理后的数据，需转换成JSON，代码在org.soul.elasticsearch.test.DataProcessTest包中。nutch解析后的数据文件为/mnt/e/official-data-07-22.txt，写入目录为/mnt/e/official-08-27/，处理函数为officialDataTest1()。读取时，根据网页的url和标题，判断同一个url或同一个标题是否出现多次。如下标题出现很多次，且毫无意义，因此凡是标题为如下字段的网页都被忽略。其中"无锡市锡山区人民政府关于对锡东新城全面开展殡葬整治工作的实施意见”这个标题的网页重复出现了4万六千多次。
\begin{verbatim}
"null","untitled","test","附件",
"热点问题汇编", "锡滨安发（2006）1号","无标题文档",
"无锡市锡山区人民政府关于对锡东新城全面开展殡葬整治工作的实施意见",
"中国无锡·无锡市人民政府·中国无锡政府门户网站·首页", 
"锡政发〔2001〕253号签发：王荣",
"中国无锡·无锡市人民政府·中国无锡政府门户网站·走进直播间·", 
"本栏目暂无内容", "锡价费函[2005]号", 
"锡药监办[2003]30号签发人：谢寿坤",
"中国无锡·无锡市政风行风热线", "中国无锡·Guu" 
\end{verbatim}
\par 大学生们整理的同义词，主要是检测每行同义词组中是否出现空格，非中文字符，",,"，“，”等，对这些不合法的字符串进行转换，最后两两比较同义词组，检测同义词组A是否完全包含同义词组B，如果完全包含，则删除同义词组B，处理后的同义词文件在soul-core工程的library/synonym-0925.txt文件中，完全包含的例子如：[冰棍, 雪条]与[冰棍, 冰棍儿, 冰棒, 冰糕, 棒冰, 雪条]。
\par 对网页标题和中文中的特殊字符需要转换，转换代码在OfficialChars类中，特殊字符比较多的是word和pdf文件。例如需要将"Ⅰ"转换成"一"，将"Ⅱ"转换成"二"，将"Ⅲ"转换成"三"，"Ⅺ"转换成"十一"。
\section{地税数据}
\par 搜索地税数据时，会遇到Excel表中小数四舍五入的情况，如搜索身份证号码：320219195609196990
\par 对整数的处理，当用户输入整数时，例如“10”，则扩展成“[10,10.0]”，前者用于查询文本域和整数域，后者只用来查询小数域。
\par 对日期的处理，当用户输入日期以查询时，索引中存在两部分日期：（1）日期域（2）文本域，文本中有一部分日期信息，如“2014年1月份开票项目”。当输入“2008年9月”，此时可以扩展成两部分：“[2008,年,9,月]”和“[2008年09月]”，前者用于查询文本域，后者用来查询日期域。
\par 当用户输入查询字符串时，是不是需要返回每个token的类型呢？如输入“10”，扩展后为“[10,10.0]”，类型分别为“[integer,float]”。当用户输入“20080909”时，扩展后为“[2008-09-09,20080909]”，类型分别为“[date,integer]”。
\par 使用CentOs搭建集群时，如果没有禁用防火墙，则两台机器间无法通信。使用命令
\begin{verbatim}
iptables -F ##隔离IP地址有效，该命令将iptables暂时清空。
其中前两个命令表示接受相应地址的端口请求，第三个命令表示拒绝所有来自其它地址的端口请求。
iptables -I INPUT 1 -p tcp --dport 9200:9300 -s 141.24.212.191,141.24.212.192 -j ACCEPT
iptables -I INPUT 3 -p tcp --dport 9200:9300 -s 141.24.24.23,141.24.24.25 -j ACCEPT
iptables -I INPUT 5 -p tcp --dport 9200:9300  -j REJECT
service iptablse save   ##保存这些防火墙规则
iptables -L ##查看防火墙规则
service iptables stop   ##关闭防火墙服务
service iptables start   ##启动防火墙服务
\end{verbatim}

