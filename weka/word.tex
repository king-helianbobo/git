\section{中文分词}
\par 词是最小的能够独立活动的有意义的语言成分，英文单词之间是以空格作为自然分界符的，而汉语是以字为基本的书写单位，词语之间没有明显的区分标记，因此，中文词语分析是中文信息处理的基础与关键。Lucene对中文的处理是基于自动切分的单字切分，或者二元切分。除此之外，还有最大切分（包括向前、向后、以及前后相结合）、最少切分、全切分等等。
\par 中文分词算法可分为三大类：基于字典、词库匹配的分词方法；基于词频度统计的分词方法和基于知识理解的分词方法。第一类方法应用词典匹配、汉语词法或其它汉语知识进行分词，如：最大匹配法、最小分词方法等。这类方法简单、分词效率较高，但汉语语言现象复杂丰富，词典的完备性、规则的一致性等问题使其难以适应开放的大规模文本的分词处理。第二类基于统计的分词方法则基于字和词的统计信息，如把相邻字间的信息、词频及相应的共现信息等应用于分词，由于这些信息是通过调查真实语料取得的，因而基于统计的分词方法具有较好的实用性。基于知识理解的分词方法主要基于句法、语法分析，并结合语义分析，通过对上下文内容所提供信息的分析对词进行定界，它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断。这类方法试图让机器具有人类的理解能力，需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式。因此目前基于知识的分词系统还处在试验阶段。
\par Bag of words，也叫做“词袋”，词袋模型假定对一个文本，忽略其词序和语法，句法，将其仅仅看做是一个词集合，或者说是词的一个组合，文本中每个词都是独立的随机变量，不依赖于其他词是否出现，这种假设虽然对自然语言进行了简化，便于模型化，但是其假定在有些情况下是不合理的，例如在新闻个性化推荐中，采用词袋模型就会出现问题。例如用户甲对“南京醉酒驾车事故”这个短语很感兴趣，采用词袋模型忽略了顺序和句法，则认为用户甲对“南京”、“醉酒”、“驾车”和“事故”感兴趣，因此可能推荐出和“南京”，“公交车”，“事故”相关的新闻，这显然不合理。
\subsection{机械分词法}
\par 基于字典、词库匹配的分词方法（机械分词法），这种方法按照一定策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行匹配，若在词典中找到某个字符串，则匹配成功。识别出一个词，根据扫描方向的不同分为正向匹配和逆向匹配。根据不同长度优先匹配的情况，分为最大（最长）匹配和最小（最短）匹配。根据与词性标注过程是否相结合，又可以分为单纯分词方法和分词与词性标注相结合的一体化方法。包含如下两种方法：
\par 最大正向匹配法(Maximum Matching Method)通常简称为MM法。其基本思想为：假定分词词典中的最长词有i个汉字字符，则用被处理文档的当前字串中的前i个字作为匹配字段，查找字典。若字典中存在这样的一个i字词，则匹配成功，匹配字段被作为一个词切分出来。如果词典中找不到这样的一个i字词，则匹配失败，将匹配字段中的最后一个字去掉，对剩下的字串重新进行匹配处理……  如此进行下去，直到匹配成功，即切分出一个词或剩余字串的长度为零为止。这样就完成了一轮匹配，然后取下一个i字字串进行匹配处理，直到文档被扫描完为止，其算法描述如下：
\begin{algorithm}
\caption{Maximum Matching Method}
\KwIn{文档doc，字典vocabulary}
\KwOut{分词序列list<word>}
当前位置计数器pos=0, n=文档长度,i=vocabulary中最长词长度\;
\For{$pos = 0; pos < n ; ;$ } {
str=从pos开始的连续i个字符，str作为匹配字段\;
\While{$str.length >1 $ \textbf{or} $str \notin vocabulary$ } {
去掉str最后一个字符\;
}
\uIf{str $\in$ vacabulary} {
匹配字段str作为一个词被切分，放入分词序列List<word>\;
pos += str的长度\;
}\Else{
str作为一个单字切分出来，放入分词序列List<word>\;
pos += 1\;
}
}
\end{algorithm}
\par 逆向最大匹配法 (Reverse Maximum Matching Method)通常简称为RMM法。RMM法的基本原理与MM法相同,不同的是分词切分的方向与MM法相反，而且使用的分词辞典也是逆序词典，其中每个词条都将按逆序方式存放。实际处理时，先将文档倒排处理，生成逆序文档，然后根据逆序词典，对逆序文档用正向最大匹配法处理。汉语中偏正结构较多，若从后向前匹配，可以适当提高精确度。逆向最大匹配法比正向最大匹配法的误差要小。统计结果表明,单纯使用正向最大匹配的错误率为$\frac{1}{169}$,单纯使用逆向最大匹配的错误率为$\frac{1}{245}$。例如切分字段“硕士研究生产”（假定最大长度为5），正向最大匹配法的结果会是“硕士研究生/产”，而逆向最大匹配法利用逆向扫描，可得到正确的分词结果“硕士/研究/生产”，其算法描述如下：
\begin{algorithm}
\caption{Reverse Maximum Matching Method}
\KwIn{文档doc，字典vocabulary}
\KwOut{分词序列List<word>}
\ForEach {$word \in vocabulary$} {
$vocabulary -= word$\;
倒排word，例如“硕士”被倒排为'士硕'\;
$vocabulary += word$\;
}
倒排doc，例如“硕士研究生产”被倒排为'产生究研士硕'\;
List<word>=调用Maximum Matching Method\;
\ForEach {$word \in List<word>$} {
倒排word;
}
\end{algorithm}
\par 最大匹配算法是一种基于分词词典的机械分词法，不能根据文档上下文的语义特征来切分词语，对词典的依赖性较大，所以在实际使用时，难免会造成一些分词错误，为了提高系统分词的准确度，可以采用正向最大匹配法和逆向最大匹配法相结合的分词方案（即双向匹配法）。
\subsection{MMSEG算法}
\par MMSEG是中文分词中一个常见的、基于词典的分词算法，简单、效果相对较好。由于它的简易直观性，实现起来不是很复杂，运行速度也比较快。总的来说现在的中文分词算法，大概可以笼统的分为两大类：一种基于词典的，一种是非基于词典的。基于词典的分词算法比较常见，比如正向／逆向最大匹配，最小切分（使一句话中的词的数量最少）等。具体使用的时候，通常是多种算法合用，或者一种为主、多种为辅，同时还会加入词性、词频等属性来辅助处理（运用某些简单的数学模型）。
\par 非基于词典的算法，一般主要是运用概率统计、机器学习方面的方法，目前常见的是CRF（Conditional random field）。此类方法可以让计算机根据现成的资料，“学习”如何分词。具体的实现可参考（http://nlp.stanford.edu/software/segmenter.shtml）。基于词典的方法，实现、部署比较容易，但是分词精度有限，且对于未登录词（词典里没有的词语）识别较差；非基于词典的方法，对未登录词识别效果较好，能够根据使用领域达到较高的分词精度，但是实现比较复杂，需要大量的前期工作。
\par MMSEG是一种基于词典的分词算法，以正向最大匹配为主，多种消除歧义的规则为辅。根据作者在原文中的阐述，对MMSEG的解释分为“匹配算法（Matching algorithm）”和“消除歧义的规则（Ambiguity resolution rules）”这两部分。“匹配算法”是说如何根据词典里保存的词语，对要切分的语句进行匹配（正向？逆向？粒度？），“消除歧义的规则”是说当一句话可以这样分，也可以那样分的时候，用什么规则来判定使用哪种分法，比如“设施和服务”这个短语，可以分成“设施/和服/务”，也可以分成“设施/和/服务”，选择哪个分词结果，就是“消除歧义的规则”的功能。
\par MMSEG的“匹配方法”有两种：Simple方法，即简单的正向匹配，根据开头的字，列出所有可能的结果。比如“一个劲儿的说话”，可以得到“一个”，“一个劲”，“一个劲儿”，“一个劲儿的”这四个匹配结果（假设这四个词都包含在词典里）。Complex方法，匹配出所有的“三个词的词组”（原文中使用了chunk，这里感觉用“词组”比较合适），即从某一既定的字为起始位置，得到所有可能的“以三个词为一组”的所有组合。比如“研究生命起源”，可以得到“研/究/生”，“研/究/生命”，“研究生/命/起源”，“研究/生命/起源”这些“词组”（根据词典，可能远不止这些，仅此举例）。“消除歧义的规则”有四个，依次使用这四个规则过滤，直到只有一种结果或者四个规则使用完毕。这个四个规则分别是：
\begin{enumerate}[(1)]
\item Maximum matching (最大匹配)，有两种情况，分别对应于使用“simple”和“complex”的匹配方法。对“simple”匹配方法，选择长度最大的词，用在上文的例子中，选择“一个劲儿的”；对“complex”匹配方法，选择长度最大的那个词组，上文的例子中即“研究生/命/起源”或者“研究/生命/起源”。
\item Largest average word length（最大平均词语长度）。经过规则(1)过滤后，如果剩余的词组超过1个，那就选择平均词语长度最大的那个（平均词长＝词组总字数／词语数量）。比如“生/活水/平”(4/3=1.33)，“生活/水/平” (4/3=1.33),“生活/水平”(4/2=2)，根据此规则，就可以确定选择“生活/水平”这个词组。
\item Smallest variance of word lengths（词语长度的最小变化率），由于词语长度的变化率可以由标准差反映，所以此处直接套用标准差公式即可。比如“研究/生命/起源”(标准差=$\sqrt{\frac{(2-2)^2+(2-2)^2+(2-2^2)}{3}}$）,“研究生/命/起源”（标准差=$\sqrt{\frac{(2-3)^2+(2-1)^2+(2-2)^2}{3}}$），于是选择“研究/生命/起源”这个词组。
\item Largest sum of degree of morphemic freedom of one/character words，其中degree of morphemic freedom可以用一个数学公式表达：$\log_e^{frequency}$，即词频的自然对数。这个规则的意思是“计算词组中的所有单字词词频的自然对数，然后将得到的值相加，取总和最大的词组”。比如：“设施/和服/务”，“设施/和/服务”，这两个词组中分别有“务”和“和”这两个单字词，假设“务”作为单字词时的频率是5，“和”作为单字词时候的频率是10，对5和10取自然对数，然后取最大值者，所以取“和”字所在的词组，即“设施/和/服务”。
\end{enumerate}
\par 取自然对数的原因在于，词组中单字词词频总和可能一样，但是实际的效果并不同，比如：A/BBB/C（单字词词频，A:3， C:7）,DD/E/F（单字词词频，E:5，F:5）表示两个词组，A、C、E、F表示不同的单字词，如果不取自然对数，单纯就词频来计算，那么这两个词组是一样的（3+7=5+5），所以这里取自然对数，以表区分（ln(3)+ln(7) < ln(5)+ln(5)）。
\par 这四个过滤规则中，如果使用simple的匹配方法，只能使用第一个规则过滤，如果使用complex的匹配方法，则四个规则都可以使用。实际使用中，一般都是使用complex的匹配方法＋四个规则过滤。（simple的匹配方法实质上就是正向最大匹配，实际中很少只用一个方法）。MMSEG分词是一个“直观”的分词方法，它把一个句子“尽可能长（指所切分的词尽可能的长）”“尽可能均匀”地切分，稍微想象一下，便感觉与中文的对称语法习惯比较相符。如果对分词精度要求不是特别高，MMSEG是一个简单、可行、快速的方法。
\par 基于词典的分词算法中，词典的结构对速度的影响是比较大的（词典的结构一般决定了匹配的方法和速度）。一般的构造词典的方法有很多，比如“首字索引＋整词二分”，将所有词语的首字用哈希做索引，然后将词体部分排序，使用二分查找。这样的方法可行，但不是最快的。对于词典匹配，trie结构一般是首选。trie也有一些变种和实现方法，对于大量静态数据的匹配（比如词典，一旦创建，便很少去修改里面的内容，故称之为“静态”），一般采用“双数组trie树（double array trie tree）”，比如Darts类库。
\par MMSEG的分词效果与词典关系较大（词典里有哪些词语，以及词频的精确度），尤其是词典中单字词的频率。可以根据使用领域，专门定制词典（比如计算机类词库，生活信息类词库，旅游类词库等），尽可能的细分词典，这样得到的分词效果会好很多。同时也可以通过词典达到一些特殊目的（地名分词等）。关于词库，可以参考“搜狗”的细胞词库（http://pinyin.sogou.com/dict/）以及其提供的语料库（可以根据其划分好的语料库，统计某一方面的词频，http://www.sogou.com/labs/resources.html）。
\subsection{基于词的频度统计的分词方法}
\par 基于词的频度统计的分词方法不依靠词典,而是将文章中任意两个字同时出现的频率进行统计,次数越高的就可能是一个词。它首先切分出与词表匹配的所有可能的词,运用统计语言模型和决策算法决定最优的切分结果。
\subsubsection{隐马尔科夫模型与维特比}
\par 机械分词算法的分词效果有限，比如下面这样一句话：产量三年中将增长两倍。按照机械分词的算法，它可能会被分成这样一种形式：产量\textbar 三年\textbar中将\textbar增长\textbar两倍。机械分词将‘中将’分成了一个词，的确‘中将’在词典中是有这么一个词，但在这句话中将它们划分成一个词显然是不合理的，于是产生了一种新的方法，基于隐马尔科夫模型的维特比算法。
\par 中文分词可以看成这样一种问题，即给定一个字串（句子），找到对应的词串（分词）。给定的字串用公式表示就是$Y= y_1,y_2,\cdots,y_m$，其中$y_i$表示一个字。需要的词串表示为：$X=x_1,x_2,\cdots,x_n$，其中$x_i$表示一个词。期望目标很简单，就是希望找到最可能的$X$，使得后验概率$P(X|Y)$最大，也就是说需要找到一种最可能的分词方法，使得在句子$Y$存在的前提条件下，该分词结果出现的概率最大。根据贝叶斯公式有：
$$P(X|Y) = \frac{P(X)P(Y|X)}{P(Y)}$$
那么为了最大化$P(X|Y)$，经过变化就变成了下面的形式，因为在给定$Y$的情况下，$P(Y)$是一个常量:
\begin{eqnarray}
\hat{X} &=& max(\frac{P(X)P(Y|X)}{P(Y)}) \\
 &\Longrightarrow& max(P(X)P(Y|X)) \\
 &=& max\{P(x_1\cdots x_n)P(y_1\cdots y_m|x_1\cdots x_n)\}\\
 &=& max\{P(x_1\cdots x_n)\}   
\end{eqnarray}
\par $P(y_1\cdots y_m|x_1\cdots x_n)=P(y_1\cdots y_{m-1}|x_1\cdots x_{n-1})P(y_m|x_n)$，事实上，$P(y_m|x_n)$的值要么是1，要么是0，因为在中文分词这种情景下，给定分词序列$x_1\cdots x_n$时，$y_1\cdots y_m$就已经确定了。由此可见，上述模型是一种马尔科夫模型，而且句子中某个词$x_n$不是凭空出现的，而和它前面的$n-1$个单词有关联，这变成了$n$阶马尔科夫模型。考虑最简单的一阶马尔科夫模型，也就是说某个分词的出现只和它紧邻的前一个分词有关，而与之前的分词无关，同时分词相对于字串的条件概率又是独立分布的（独立输出假设）。那么有如下公式：
\begin{eqnarray}
P(x_1x_2\cdots x_n)&=&P(x_1x_2\cdots x_{n-1})P(x_n|x_1x_2\cdots x_{n-1})\\
&=&P(x_1x_2\cdots x_{n-1})P(x_n|x_{n-1})\\
&=&P(x_1x_2\cdots x_{n-2})P(x_{n-1}|x_{n-2})P(x_n|x_{n-1})\\
&=&P(x_1)P(x_2|x_1)\cdots P(x_n|x_{n-1})
\end{eqnarray}
\par 其中，$X$（分词）是隐变量，$Y$（字串）是观测变量，最终的目标是找到一个词串$x_1\cdots x_n$，使得能最大化$P(x_1x_2\cdots x_n)$。由于当给定$P(x_1x_2\cdots x_{n-1})$后，再知道$P(x_n|x_{n-1})$后，就可以计算出$P(x_1x_2\cdots x_n)$，因此计算最大概率的方法实际上是一种动态规划算法，先计算$P(x_0x_1)$，再计算$P(x_0x_1x_2)$，以此类推。对$P(x_n|x_{n-1})$的计算可通过训练大量语料（如人民日报）获得，例如求'$P$(总理|国务院)'，可先统计'国务院'的出现次数$count$，然后统计'总理'紧挨着'国务院'的出现次数$count_1$，则$P$(总理|国务院)=$\frac{count_1}{count}$。
\subsubsection{维特比算法}
\par 想象一个乡村诊所，村民有着非常理想化的特性，要么健康要么发烧，他们只有问诊所的医生才能知道是否发烧。聪明的医生通过询问病人的感觉诊断他们是否发烧。村民只回答他们感觉正常、头晕或冷。假设一个病人每天来到诊所并告诉医生他的感觉。医生相信病人的健康状况如同一个离散马尔可夫链。病人的状态有两种“健康”和“发烧”，但医生不能直接观察到，这意味着状态对他是“隐含”的。每天病人会告诉医生自己有以下几种由他的健康状态决定的感觉的一种：正常、冷或头晕。这些是观察结果。整个系统为一个隐马尔可夫模型(HMM)。医生知道村民的总体健康状况，还知道发烧和没发烧的病人通常会抱怨什么症状。
\begin{verbatim}
states = ('Healthy', 'Fever')
observations = ('normal', 'cold', 'dizzy')
start_probability = {'Healthy': 0.6, 'Fever': 0.4}
transition_probability = {
   'Healthy' : {'Healthy': 0.7, 'Fever': 0.3},
   'Fever' : {'Healthy': 0.4, 'Fever': 0.6}}
emission_probability = {
   'Healthy' : {'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1},
   'Fever' : {'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6}}
\end{verbatim}
\par 这段代码中,起始概率start\_probability表示病人第一次到访时医生认为其所处的HMM状态，他唯一知道的是病人倾向于是健康的。这里用到的特定概率分布不是均衡的，如转移概率大约是{'Healthy': 0.57, 'Fever': 0.43}。转移概率transition\_probability表示潜在的马尔可夫链中健康状态的变化。当天健康的病人仅有30\%的机会第二天会发烧。放射概率emission\_probability表示每天病人感觉的可能性，即$P(normal|Healthy)=0.5$,$P(cold|Healthy)=0.4$,$P(dizzy|Healthy)=0.1$。假如他是健康的，50\%会感觉正常。病人连续三天看医生，医生发现第一天他感觉正常，第二天感觉冷，第三天感觉头晕。于是医生产生了一个问题：怎样的健康状态序列最能够解释这些观察结果，维特比算法可以回答这个问题。
\begin{algorithm}[H]
\caption{维特比算法}
\KwIn{$obs,States,startP,transP,emitP$}
\KwOut{最大概率$prob$,健康状态序列$P[]$}
$V[][] = 0; Path[][]=\emptyset$\;
\For{$t=0;t<len;t++$} {
\uIf{$t==0$}{
\For{$y_i \in States$}{
$V[0][y_i] = startP[y_i] * P(obs[0] | y_i)$ \;
$Path[0][y_i] = \emptyset$\;
}}\Else{
\For{$y_i \in States$}{
$Path[t][y_i] = y_i$\;
$V[t][y_i]=0$\;
\For{$y_j \in States$} {
\If{$V[t][y_i] \le V[t-1][y_j] \times P(y_i|y_j) \times P(obs[t]|y_i) $}{
$V[t][y_i]=V[t-1][y_j] \times P(y_i|y_j)\times P(obs[t]|y_i) $\;
$Path[t][y_i] = y_j$\;}
}}}}
$prob = max(V[len-1][0],V[len-1][1],\cdots)$\;
$y = y_i$，其中$V[len-1][y_i]$为$V[len-1][*]$中最大值\;
定义堆栈$stack = \emptyset$,$stack.push(y)$\;
\For{$t=len-1;t > 0; t--$} {
$y=Path[t][y]$\;
$stack.push(y)$\;
}
\Repeat{$stack.empty()$}{
$P.add(stack.pop())$\;
}
\Return{$prob,P[]$}
\end{algorithm}
\par viterbi算法中，obs为观察结果序列，如['normal','cold','dizzy']，states为一组隐含状态，start\_p 为起始状态概率，trans\_p为转移概率，而emit\_p为放射概率。维特比算法揭示了观察结果['normal','cold','dizzy']最有可能由状态序列['Healthy','Healthy','Fever']产生。换句话说，对于观察到的活动,病人第一天感到正常，第二天感到冷时都是健康的，而第三天发烧了。在实现维特比算法时需注意许多编程语言使用浮点数计算，当p很小时可能会导致结果下溢。避免这一问题的常用技巧是在整个计算过程中使用对数概率，在对数系统中也使用了同样的技巧。当算法结束时，可以通过适当的幂运算获得精确结果。
\subsection{条件随机场}
\par 与最大熵模型相似，条件随机场（Conditional random fields，CRFs）是一种机器学习模型，在自然语言处理的许多领域（如词性标注、中文分词、命名实体识别等）都有比较好的应用效果。条件随机场最早由John D. Lafferty提出，其也是Brown90的作者之一，和贾里尼克相似，在离开IBM后他去了卡耐基梅隆大学继续搞学术研究，2001年以第一作者的身份发表了CRF的经典论文 “Conditional random fields: Probabilistic models for segmenting and labeling sequence data”。
\par 关于条件随机场的参考文献及其他资料，Hanna Wallach在05年整理和维护的这个页面(http://www.inference.phy.cam.ac.uk/hmw26/crf/)非常不错，其中涵盖了自01年CRF提出以来的很多经典论文（不过似乎只到05年，之后并未更新）以及几个相关的工具包(不过也没有包括CRF++），但是仍然非常值得入门条件随机场的读者参考。
\par 标记问题解决分词：就是将词语开始和结束的字标记出来，就能对一个句子完成分词，假设使用两个标记B (开始)，E(结束)对句子进行处理，如：“民主是普世价值”，民/B 主/E 是/B 普/B 世/E 价/B 值/E, 这样标记明确，分词结果就明确了。下面的问题是如何找到最好的标记结果：知道如何用标记的方式解决分词，那么怎么为一个句子找到一个最好的标记序列呢，CRF为这样的问题提供了一个解决方案，对于输入序列$X_1,X_2,\cdots,X_n$($X_i$代表一个字)，求这个输入序列下，某个标记序列$(Y_1,Y_2,\cdots,Y_n)$的概率最大值（每个$X_i$有一个相应的标记$Y_i$）。
\par 这里用一个例子来说明，公式为：$P(y|x,\lambda)=\sum _j \lambda_jF_j(y,x)/Z(x)$。假设使用4种标记，B代表开始，O代表单独成词，M代表词语中间的字，E代表词的结束。一元特征：其中$V_{-1}$表示当前字的前一个字，$V_0$表示当前字，$V_1$表示当前字的后一个字。二元特征：各标记间的转移特征。句子如下：
{\color{red}
\begin{tabbing}
%%如下一行称为样本行，样本行末尾不能以\\结尾，必须以\kill结束，这行仅用来设置制表位，不被显示
民\hspace*{20bp} \=民\hspace*{20bp} \=民\hspace*{20bp} \=民\hspace*{20bp} \=民\hspace*{20bp} \=民\hspace*{20bp} \=民\hspace*{20bp} \=民\kill
%%tabbing环境不会自动分行，换行必须以\\实现
民 \>主 \>是 \>普 \>世 \>价 \>值\\
B \>B \>B \>B \>B \>B \>B\\
O  \>O   \>O   \>O   \>O   \>O   \>O\\
M  \>M   \>M   \>M   \>M   \>M   \>M\\
E   \>E   \>E   \>E   \>E   \>E   \>E\\
\end{tabbing}}
\par Viterbe算法就是在以上由标记组成的图中搜索一条最优路径。对于每一列的每一个标记，我们都要计算到达该标记的分数，这个分数由三部分组成，它本身的一元特征权重W，它前面一个字标记的路径分数PreScore，前面一个字标记到当前标记转移特征权重TransW。
\begin{enumerate}[(1)]
\item 计算第一列的分数score,对‘民’来说，要算它作为B,O,M,E的分数，因为是第一列，所以PreSocre和TransW都是0，只需要计算在一元特征下的权重。对于标记\textbf{B}，它的Score为$S_{1B}$=$W_{1B}$=w(null,民,B)+w(民,B)+w(民,B,主)。意思是：\textbf{w(null,民,B)}，当前字为‘民’标记为B，前面一个字为空；\textbf{w(民,B)}：当前字为‘民’，标记为B；\textbf{w(民,B,主)}：当前字为’民’，标记为B，当前字的后一个字为‘主’。特征权重都是在训练时得到的，对于标记O,M,E，一样要计算$W_{1O}$（标记为O的权重），$W_{1M}$（标记为M的权重），$W_{1E}$（标记为E的权重）,从而得到分数$S_{1O}$,$S_{1M}$,$S_{1E}$。
\item 对于第二列，首先要计算每个标记的一元权重$W_{2B}$，$W_{2O}$,$W_{2M}$,$W_{2E}$。对于B，到达该标记的最大分数为：$$S_{2B}=Max\{(S_{1B}+V_{BB}),(S_{1O}+V_{OB}),(S_{1M}+V_{MB}),(S_{1E}+V_{EB})\}+W_{2B}$$
其中$V_{BB}$为标记B到标记B的转移特征的权重，这个也由训练得到。同样对于第二列的O,M,E也要计算$S_{2O}$，$S_{2M}$，$S_{2E}$。
\item 一直计算到最后一列，从而获得每个字的所有标记分数，得到$S_{7B}$，$S_{7O}$，$S_{7M}$，$S_{7E}$。比较这四个值中的最大值，即为最优路径的分数，然后以该值的标记点为起始点，回溯得到最优路径（因此在计算过程中，要记录到达该标记的前一个标记，用于回溯）。
\end{enumerate}
\subsection{ICTCLAS汉语词性}
\begin{enumerate}[(1)]
\item a 形容词，取英语形容词adjective的第1个字母（共34439个）。
\item ad 直接作状语的形容词，形容词代码a和副词代码d并在一起。（5899）
\item ag 形容词性语素，形容词代码为a，语素代码ｇ前面置以A。（311）
\item an 名形词，具有名词功能的形容词。形容词代码a和名词代码n并在一起。（2838）
\item b 区别词，取汉字“别”的声母。（8734）
\item c 连词，取英语连词conjunction的第1个字母。（25473）
\item d：副词，取adverb的第2个字母，因其第1个字母已用于形容词。（47714），dg：副词性语素，副词代码为d，语素代码g前面置以d。
\item e 感叹词，取英语exclamation的第1个字母。
\item f 方位词，取汉字“方”的首字母（17248）。
\item g 语素，绝大多数语素都能作为合成词的“词根”，取汉字“根”的声母。（这里没有对应的）
\item h：前接成分，取英语head的第1个字母。（48个）;k：后接成分（类似于h，共958个）。
\item i，成语，取英语成语idiom的首字母（5001个）。
\item j，简略字，取汉字“简”的声母（简称，比如“驾照”，“省纪委”，共10293个），但某些词，比如“中纪委”却不具备这个词性。
\item l 习用语，习用语尚未成为成语，有点“临时性”，取“临”的声母。
\item m，数字词，取numerical的第3个字母，nu已有他用（共41036个）。
\item n 名词，取英语名词noun的第1个字母（很多，共237124个）。
\item ng 名语素，名词性语素。名词代码为n，语素代码ｇ前面置以N。（4497）
\item nr 人名，名词代码n和“人(ren)”的声母并在一起。（20061）
\item ns 地名，名词代码n和处所词代码s(space)并在一起。（27777）
\item nt 机构团体，“团”的声母为t，名词代码n和t并在一起。（3565）
\item nz 专有名词，“专”的首字母为z，名词代码n和z并在一起。（3728）
\item o 拟声词，取英语拟声词onomatopoeia的第1个字母。
\item p 介词，取英语介词prepositional的第1个字母。（39906）
\item q 量词，取英语quantity的第1个字母（24236个，如“倍”，“个”，“斗”）。
\item r 代词，代名词pronoun的第2个字母，因p已用于介词（32367个）。
\item s 处所词，取英语space的第1个字母。
\item t：时间词，取英语time的第1个字母。tg：时语素，时间词性语素。时间词代码为t,在语素的代码g前面置以t。
\item u 助词，取英语助词auxiliary。
\item v 动词取英语动词verb的第一个字母。
\item vd：副动词，直接作状语的动词。动词和副词的代码并在一起。
\item vg：动词性语素。动词代码为v。在语素的代码g前面置以v。
\item vn 名动词，指具有名词功能的动词。动词和名词的代码并在一起。
\item w 标点符号（共173046个）。
\item x 非语素字，非语素字只是一个符号，字母$x$通常用于代表未知数、符号。
\item y 语气词，取汉字“语”的声母。
\item z 状态词，取汉字“状”的声母的前一个字母。
\end{enumerate}
\subsection{实施方案}
\begin{enumerate}[(1)]
\item 繁简转换，（问题不难，使用繁简转换表）。
\item 拼音转汉字，（属于优化的问题，现在没必要做）。
\item 同音词拼写错误，先把汉字转成拼音，然后进行可能的比对，比如中华人名共和国-->中华人民共和国，这个已经实现。
\item 英文拼写错误，（暂时不作考虑，可以考虑使用开源的库）。
\item 形近词错误，（这个需要吗，当用户使用五笔或记错某个字的样子时）
\end{enumerate}
\subsubsection{拼写错误检查}
\par 拼音转汉字想法是较为直接的，建立一个以拼音为term的查询词索引，posting list中只保存查询频率最高的K个查询词，如
\begin{table}[h]
  \centering
  \begin{tabular}{|l|l|}
\hline
jiujingkaoyan &“久经考验”,“酒精考验”\\ \hline
zhijiucaotang & “子九草堂', “子久草堂” \\ \hline
xufuniuza & “徐福牛杂”,“许府牛杂”,“徐府牛杂”\\ \hline
shoujichongzhi&“手机冲值”, “手机充值”\\ \hline
  \end{tabular}
\end{table}
\par 这一步可以在自动提示中使用，但自动提示与它的区别是，自动提示在拼音输入了一部分的情况下也要提示，比如输入 “xufu”就要提示“许府牛杂”。
\par 同音词拼写错误也基于同样的想法，但是需要一个可能出错的查询词列表，这个列表可以为借鉴于下列几种情况：
\begin{enumerate}[(1)]
\item 以carot为例，返回有carot的文档，也返回一些包含纠错后的term carrot和torot的文档。
\item 与(1)相似，但仅当carot不在词典中时，返回纠错后的结果。
\item 与(1)相似，但仅当包含carot的文档数小于一个预定义的阈值时，即当原始查询返回文档数小于预定义的阈值时，搜索引擎给出纠错后的词列表。
\end{enumerate}
\par 情况(1)相当于是对所有查询都进行纠错处理，发现那些搜索比较少的，就给出一个纠错提示，比如“天浴”搜索次数比较少，而“天娱”搜索次数比较多，那么在用户搜索“天浴”时就提示“天娱”，即使“天浴”也是一个正常的查询词。情况(2)就是当查询没有获得文档，才对它进行纠错处理，然后查询相应结果。情况(3)是一个查询它返回的文档数少于一个预定义阈值时，才进行纠错处理。
\par 英文拼写错误，在lucene中已有贡献者实现了spellchecker模块，主要算法有：Jaro Winkler distance，Levenstein Distance(Edit Distance)，NGram Distance。但Lucene中的实现过于简单，使用两两比较，时间复杂性是$O(n^2)$。
\par 形近字错误，形近字一般是用户记错了形声字，或是使用五笔的用户输入错误。在网上可以下载SunWb\_mb文件，它里面包含五笔的编码和笔画的编码，但字根比如“马”比“口”笔画更多，也更有代表性，但在这种方法中却是相同的。
\par 方言纠错,可以用soudex进行纠错
\subsubsection{网站群的一些问题}
\par 政府网站中的特殊字符需要进行转换。
\par 当对政府网站中的网页建立索引时，如果使用url作为文档id，相同内容的文档可能出现多次。下面两个url仅仅是字符大小写不同，却作为不同的id出现。解决办法是文档id一律小写。
\begin{verbatim}
http://www.wuxi.gov.cn/WebPortal/AskAnswer/Gov_AskAnswer_Info?
AnswerID=9174e3c9-4bb6-41cd-ba75-6465a3aa490c
http://www.wuxi.gov.cn/WEBPORTAL/AskAnswer/Gov_AskAnswer_Info?
AnswerID=9174e3c9-4bb6-41cd-ba75-6465a3aa490c
\end{verbatim}
\par 下面两个url均包含了“$\backslash$”字符，其后跟随普通ASCII字符时，转化成一种转义字符，作为文档id会出错。
\begin{verbatim}
http://www.wuxi.gov.cn/WEBPORTAL/ChiefHall/ChiefHallInfoDetailsXK?SystemID
=4028818a28aff36d0128b3861a090c76&ChiefHallType=xk360ChromeURL\Shell\Open\Command
http://www.wuxi.gov.cn/WEBPORTAL/ChiefHall/ChiefHallInfoDetailsXK?SystemID
=4028818a2fbfee4b012fc2fe529909ac&ChiefHallType=xk360ChromeURL\Shell\Open\Command
\end{verbatim}
\par 爬虫获得的网页内容中，如标题为《京杭大运河无锡段》，而内容为《京杭大运河无锡段 发布时间： 2011年11月23日 修改时间： 2013年10月22日 [ 大 中 小 ] 浏览次数：》，这部分内容重复，影响了检索的相关性，需将其过滤。
\par “春涛”这个词，结合上下文，分词结果不同，下面两个例子，前者分成了“春，涛”，后者分成了“春涛”，还有词，比如：“易视腾”，“买卖宝”。
\begin{verbatim}
curl -XGET 'namenode:9200/official_mini/_analyze?analyzer=soul_index&pretty' -d '登太湖仙岛、观鼋渚春涛'
curl -XGET 'namenode:9200/official_mini/_analyze?analyzer=soul_index&pretty' -d '鼓浪屿上听春涛'
\end{verbatim}
\par 对2014年3月18号爬取的网站群数据，作分词处理后，共有164976个不同的词，总共的词个数是161630251个，使用word2vec代码如下，最后一行代码对SogouR.txt进行编码转换。
\begin{verbatim}
./word2vec -train /mnt/f/b.txt -output vectors.bin -cbow 0 -size 200 -window 7 -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 1
./distance vectors.bin
cat SogouR.txt | iconv -f gbk -t utf8 -c > SogouR-utf8.txt
\end{verbatim}
