\section{中文分词}
\par 词是最小的能够独立活动的有意义的语言成分，英文单词之间是以空格作为自然分界符的，而汉语是以字为基本的书写单位，词语之间没有明显的区分标记，因此，中文词语分析是中文信息处理的基础与关键。Lucene对中文的处理是基于自动切分的单字切分，或者二元切分。除此之外，还有最大切分（包括向前、向后、以及前后相结合）、最少切分、全切分等等。
\par 中文分词算法可分为三大类：基于字典、词库匹配的分词方法；基于词频度统计的分词方法和基于知识理解的分词方法。第一类方法应用词典匹配、汉语词法或其它汉语知识进行分词，如：最大匹配法、最小分词方法等。这类方法简单、分词效率较高，但汉语语言现象复杂丰富，词典的完备性、规则的一致性等问题使其难以适应开放的大规模文本的分词处理。第二类基于统计的分词方法则基于字和词的统计信息，如把相邻字间的信息、词频及相应的共现信息等应用于分词，由于这些信息是通过调查真实语料取得的，因而基于统计的分词方法具有较好的实用性。基于知识理解的分词方法主要基于句法、语法分析，并结合语义分析，通过对上下文内容所提供信息的分析对词进行定界，它通常包括三个部分：分词子系统、句法语义子系统、总控部分。在总控部分的协调下，分词子系统可以获得有关词、句子等的句法和语义信息来对分词歧义进行判断。这类方法试图让机器具有人类的理解能力，需要使用大量的语言知识和信息。由于汉语语言知识的笼统、复杂性，难以将各种语言信息组织成机器可直接读取的形式。因此目前基于知识的分词系统还处在试验阶段。
\par Bag of words，也叫做“词袋”，词袋模型假定对一个文本，忽略其词序和语法，句法，将其仅仅看做是一个词集合，或者说是词的一个组合，文本中每个词都是独立的随机变量，不依赖于其他词是否出现，这种假设虽然对自然语言进行了简化，便于模型化，但是其假定在有些情况下是不合理的，例如在新闻个性化推荐中，采用词袋模型就会出现问题。例如用户甲对“南京醉酒驾车事故”这个短语很感兴趣，采用词袋模型忽略了顺序和句法，则认为用户甲对“南京”、“醉酒”、“驾车”和“事故”感兴趣，因此可能推荐出和“南京”，“公交车”，“事故”相关的新闻，这显然不合理。
\subsection{机械分词法}
\par 基于字典、词库匹配的分词方法（机械分词法），这种方法按照一定策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行匹配，若在词典中找到某个字符串，则匹配成功。识别出一个词，根据扫描方向的不同分为正向匹配和逆向匹配。根据不同长度优先匹配的情况，分为最大（最长）匹配和最小（最短）匹配。根据与词性标注过程是否相结合，又可以分为单纯分词方法和分词与词性标注相结合的一体化方法。包含如下两种方法：
\par 最大正向匹配法(Maximum Matching Method)通常简称为MM法。其基本思想为：假定分词词典中的最长词有i个汉字字符，则用被处理文档的当前字串中的前i个字作为匹配字段，查找字典。若字典中存在这样的一个i字词，则匹配成功，匹配字段被作为一个词切分出来。如果词典中找不到这样的一个i字词，则匹配失败，将匹配字段中的最后一个字去掉，对剩下的字串重新进行匹配处理……  如此进行下去，直到匹配成功，即切分出一个词或剩余字串的长度为零为止。这样就完成了一轮匹配，然后取下一个i字字串进行匹配处理，直到文档被扫描完为止，其算法描述如下：
\begin{algorithm}
\caption{Maximum Matching Method}
\KwIn{文档doc，字典vocabulary}
\KwOut{分词序列list<word>}
当前位置计数器pos=0, n=文档长度,i=vocabulary中最长词长度\;
\For{$pos = 0; pos < n ; ;$ } {
str=从pos开始的连续i个字符，str作为匹配字段\;
\While{$str.length >1 $ \textbf{or} $str \notin vocabulary$ } {
去掉str最后一个字符\;
}
\uIf{str $\in$ vacabulary} {
匹配字段str作为一个词被切分，放入分词序列List<word>\;
pos += str的长度\;
}\Else{
str作为一个单字切分出来，放入分词序列List<word>\;
pos += 1\;
}
}
\end{algorithm}
\par 逆向最大匹配法 (Reverse Maximum Matching Method)通常简称为RMM法。RMM法的基本原理与MM法相同,不同的是分词切分的方向与MM法相反，而且使用的分词辞典也是逆序词典，其中每个词条都将按逆序方式存放。实际处理时，先将文档倒排处理，生成逆序文档，然后根据逆序词典，对逆序文档用正向最大匹配法处理。汉语中偏正结构较多，若从后向前匹配，可以适当提高精确度。逆向最大匹配法比正向最大匹配法的误差要小。统计结果表明,单纯使用正向最大匹配的错误率为$\frac{1}{169}$,单纯使用逆向最大匹配的错误率为$\frac{1}{245}$。例如切分字段“硕士研究生产”（假定最大长度为5），正向最大匹配法的结果会是“硕士研究生/产”，而逆向最大匹配法利用逆向扫描，可得到正确的分词结果“硕士/研究/生产”，其算法描述如下：
\begin{algorithm}
\caption{Reverse Maximum Matching Method}
\KwIn{文档doc，字典vocabulary}
\KwOut{分词序列List<word>}
\ForEach {$word \in vocabulary$} {
$vocabulary -= word$\;
倒排word，例如“硕士”被倒排为'士硕'\;
$vocabulary += word$\;
}
倒排doc，例如“硕士研究生产”被倒排为'产生究研士硕'\;
List<word>=调用Maximum Matching Method\;
\ForEach {$word \in List<word>$} {
倒排word;
}
\end{algorithm}
\par 最大匹配算法是一种基于分词词典的机械分词法，不能根据文档上下文的语义特征来切分词语，对词典的依赖性较大，所以在实际使用时，难免会造成一些分词错误，为了提高系统分词的准确度，可以采用正向最大匹配法和逆向最大匹配法相结合的分词方案（即双向匹配法）。
\subsection{MMSEG算法}
\par MMSEG是中文分词中一个常见的、基于词典的分词算法，简单、效果相对较好。由于它的简易直观性，实现起来不是很复杂，运行速度也比较快。总的来说现在的中文分词算法，大概可以笼统的分为两大类：一种基于词典的，一种是非基于词典的。基于词典的分词算法比较常见，比如正向／逆向最大匹配，最小切分（使一句话中的词的数量最少）等。具体使用的时候，通常是多种算法合用，或者一种为主、多种为辅，同时还会加入词性、词频等属性来辅助处理（运用某些简单的数学模型）。
\par 非基于词典的算法，一般主要是运用概率统计、机器学习方面的方法，目前常见的是CRF（Conditional random field）。此类方法可以让计算机根据现成的资料，“学习”如何分词。具体的实现可参考（http://nlp.stanford.edu/software/segmenter.shtml）。基于词典的方法，实现、部署比较容易，但是分词精度有限，且对于未登录词（词典里没有的词语）识别较差；非基于词典的方法，对未登录词识别效果较好，能够根据使用领域达到较高的分词精度，但是实现比较复杂，需要大量的前期工作。
\par MMSEG是一种基于词典的分词算法，以正向最大匹配为主，多种消除歧义的规则为辅。根据作者在原文中的阐述，对MMSEG的解释分为“匹配算法（Matching algorithm）”和“消除歧义的规则（Ambiguity resolution rules）”这两部分。“匹配算法”是说如何根据词典里保存的词语，对要切分的语句进行匹配（正向？逆向？粒度？），“消除歧义的规则”是说当一句话可以这样分，也可以那样分的时候，用什么规则来判定使用哪种分法，比如“设施和服务”这个短语，可以分成“设施/和服/务”，也可以分成“设施/和/服务”，选择哪个分词结果，就是“消除歧义的规则”的功能。
\par MMSEG的“匹配方法”有两种：Simple方法，即简单的正向匹配，根据开头的字，列出所有可能的结果。比如“一个劲儿的说话”，可以得到“一个”，“一个劲”，“一个劲儿”，“一个劲儿的”这四个匹配结果（假设这四个词都包含在词典里）。Complex方法，匹配出所有的“三个词的词组”（原文中使用了chunk，这里感觉用“词组”比较合适），即从某一既定的字为起始位置，得到所有可能的“以三个词为一组”的所有组合。比如“研究生命起源”，可以得到“研/究/生”，“研/究/生命”，“研究生/命/起源”，“研究/生命/起源”这些“词组”（根据词典，可能远不止这些，仅此举例）。“消除歧义的规则”有四个，依次使用这四个规则过滤，直到只有一种结果或者四个规则使用完毕。这个四个规则分别是：
\begin{enumerate}[(1)]
\item Maximum matching (最大匹配)，有两种情况，分别对应于使用“simple”和“complex”的匹配方法。对“simple”匹配方法，选择长度最大的词，用在上文的例子中，选择“一个劲儿的”；对“complex”匹配方法，选择长度最大的那个词组，上文的例子中即“研究生/命/起源”或者“研究/生命/起源”。
\item Largest average word length（最大平均词语长度）。经过规则(1)过滤后，如果剩余的词组超过1个，那就选择平均词语长度最大的那个（平均词长＝词组总字数／词语数量）。比如“生/活水/平”(4/3=1.33)，“生活/水/平” (4/3=1.33),“生活/水平”(4/2=2)，根据此规则，就可以确定选择“生活/水平”这个词组。
\item Smallest variance of word lengths（词语长度的最小变化率），由于词语长度的变化率可以由标准差反映，所以此处直接套用标准差公式即可。比如“研究/生命/起源”(标准差=$\sqrt{\frac{(2-2)^2+(2-2)^2+(2-2^2)}{3}}$）,“研究生/命/起源”（标准差=$\sqrt{\frac{(2-3)^2+(2-1)^2+(2-2)^2}{3}}$），于是选择“研究/生命/起源”这个词组。
\item Largest sum of degree of morphemic freedom of one/character words，其中degree of morphemic freedom可以用一个数学公式表达：$\log_e^{frequency}$，即词频的自然对数。这个规则的意思是“计算词组中的所有单字词词频的自然对数，然后将得到的值相加，取总和最大的词组”。比如：“设施/和服/务”，“设施/和/服务”，这两个词组中分别有“务”和“和”这两个单字词，假设“务”作为单字词时的频率是5，“和”作为单字词时候的频率是10，对5和10取自然对数，然后取最大值者，所以取“和”字所在的词组，即“设施/和/服务”。
\end{enumerate}
\par 取自然对数的原因在于，词组中单字词词频总和可能一样，但是实际的效果并不同，比如：A/BBB/C（单字词词频，A:3， C:7）,DD/E/F（单字词词频，E:5，F:5）表示两个词组，A、C、E、F表示不同的单字词，如果不取自然对数，单纯就词频来计算，那么这两个词组是一样的（3+7=5+5），所以这里取自然对数，以表区分（ln(3)+ln(7) < ln(5)+ln(5)）。
\par 这四个过滤规则中，如果使用simple的匹配方法，只能使用第一个规则过滤，如果使用complex的匹配方法，则四个规则都可以使用。实际使用中，一般都是使用complex的匹配方法＋四个规则过滤。（simple的匹配方法实质上就是正向最大匹配，实际中很少只用一个方法）。MMSEG分词是一个“直观”的分词方法，它把一个句子“尽可能长（指所切分的词尽可能的长）”“尽可能均匀”地切分，稍微想象一下，便感觉与中文的对称语法习惯比较相符。如果对分词精度要求不是特别高，MMSEG是一个简单、可行、快速的方法。
\par 基于词典的分词算法中，词典的结构对速度的影响是比较大的（词典的结构一般决定了匹配的方法和速度）。一般的构造词典的方法有很多，比如“首字索引＋整词二分”，将所有词语的首字用哈希做索引，然后将词体部分排序，使用二分查找。这样的方法可行，但不是最快的。对于词典匹配，trie结构一般是首选。trie也有一些变种和实现方法，对于大量静态数据的匹配（比如词典，一旦创建，便很少去修改里面的内容，故称之为“静态”），一般采用“双数组trie树（double array trie tree）”，比如Darts类库。
\par MMSEG的分词效果与词典关系较大（词典里有哪些词语，以及词频的精确度），尤其是词典中单字词的频率。可以根据使用领域，专门定制词典（比如计算机类词库，生活信息类词库，旅游类词库等），尽可能的细分词典，这样得到的分词效果会好很多。同时也可以通过词典达到一些特殊目的（地名分词等）。关于词库，可以参考“搜狗”的细胞词库（http://pinyin.sogou.com/dict/）以及其提供的语料库（可以根据其划分好的语料库，统计某一方面的词频，http://www.sogou.com/labs/resources.html）。
\subsection{基于词的频度统计的分词方法}
\par 基于词的频度统计的分词方法不依靠词典,而是将文章中任意两个字同时出现的频率进行统计,次数越高的就可能是一个词。它首先切分出与词表匹配的所有可能的词,运用统计语言模型和决策算法决定最优的切分结果。
\subsubsection{隐马尔科夫模型与维特比}
\par 机械分词算法的分词效果有限，比如下面这样一句话：产量三年中将增长两倍。按照机械分词的算法，它可能会被分成这样一种形式：产量\textbar 三年\textbar中将\textbar增长\textbar两倍。机械分词将‘中将’分成了一个词，的确‘中将’在词典中是有这么一个词，但在这句话中将它们划分成一个词显然是不合理的，于是产生了一种新的方法，基于隐马尔科夫模型的维特比算法。
\par 中文分词可以看成这样一种问题，即给定一个字串（句子），找到对应的词串（分词）。给定的字串用公式表示就是$Y= y_1,y_2,\cdots,y_m$，其中$y_i$表示一个字。需要的词串表示为：$X=x_1,x_2,\cdots,x_n$，其中$x_i$表示一个词。期望目标很简单，就是希望找到最可能的$X$，使得后验概率$P(X|Y)$最大，也就是说需要找到一种最可能的分词方法，使得在句子$Y$存在的前提条件下，该分词结果出现的概率最大。根据贝叶斯公式有：
$$P(X|Y) = \frac{P(X)P(Y|X)}{P(Y)}$$
那么为了最大化$P(X|Y)$，经过变化就变成了下面的形式，因为在给定$Y$的情况下，$P(Y)$是一个常量:
\begin{eqnarray}
\hat{X} &=& max(\frac{P(X)P(Y|X)}{P(Y)}) \\
 &\Longrightarrow& max(P(X)P(Y|X)) \\
 &=& max\{P(x_1\cdots x_n)P(y_1\cdots y_m|x_1\cdots x_n)\}\\
 &=& max\{P(x_1\cdots x_n)\}   
\end{eqnarray}
\par $P(y_1\cdots y_m|x_1\cdots x_n)=P(y_1\cdots y_{m-1}|x_1\cdots x_{n-1})P(y_m|x_n)$，事实上，$P(y_m|x_n)$的值要么是1，要么是0，因为在中文分词这种情景下，给定分词序列$x_1\cdots x_n$时，$y_1\cdots y_m$就已经确定了。由此可见，上述模型是一种马尔科夫模型，而且句子中某个词$x_n$不是凭空出现的，而和它前面的$n-1$个单词有关联，这变成了$n$阶马尔科夫模型。考虑最简单的一阶马尔科夫模型，也就是说某个分词的出现只和它紧邻的前一个分词有关，而与之前的分词无关，同时分词相对于字串的条件概率又是独立分布的（独立输出假设）。那么有如下公式：
\begin{eqnarray}
P(x_1x_2\cdots x_n)&=&P(x_1x_2\cdots x_{n-1})P(x_n|x_1x_2\cdots x_{n-1})\\
&=&P(x_1x_2\cdots x_{n-1})P(x_n|x_{n-1})\\
&=&P(x_1x_2\cdots x_{n-2})P(x_{n-1}|x_{n-2})P(x_n|x_{n-1})\\
&=&P(x_1)P(x_2|x_1)\cdots P(x_n|x_{n-1})
\end{eqnarray}
\par 其中，$X$（分词）是隐变量，$Y$（字串）是观测变量，最终的目标是找到一个词串$x_1\cdots x_n$，使得能最大化$P(x_1x_2\cdots x_n)$。由于当给定$P(x_1x_2\cdots x_{n-1})$后，再知道$P(x_n|x_{n-1})$后，就可以计算出$P(x_1x_2\cdots x_n)$，因此计算最大概率的方法实际上是一种动态规划算法，先计算$P(x_0x_1)$，再计算$P(x_0x_1x_2)$，以此类推。对$P(x_n|x_{n-1})$的计算可通过训练大量语料（如人民日报）获得，例如求'$P$(总理|国务院)'，可先统计'国务院'的出现次数$count$，然后统计'总理'紧挨着'国务院'的出现次数$count_1$，则$P$(总理|国务院)=$\frac{count_1}{count}$。
\subsubsection{维特比算法}
\par 想象一个乡村诊所，村民有着非常理想化的特性，要么健康要么发烧，他们只有问诊所的医生才能知道是否发烧。聪明的医生通过询问病人的感觉诊断他们是否发烧。村民只回答他们感觉正常、头晕或冷。假设一个病人每天来到诊所并告诉医生他的感觉。医生相信病人的健康状况如同一个离散马尔可夫链。病人的状态有两种“健康”和“发烧”，但医生不能直接观察到，这意味着状态对他是“隐含”的。每天病人会告诉医生自己有以下几种由他的健康状态决定的感觉的一种：正常、冷或头晕。这些是观察结果。整个系统为一个隐马尔可夫模型(HMM)。医生知道村民的总体健康状况，还知道发烧和没发烧的病人通常会抱怨什么症状。
\begin{verbatim}
states = ('Healthy', 'Fever')
observations = ('normal', 'cold', 'dizzy')
start_probability = {'Healthy': 0.6, 'Fever': 0.4}
transition_probability = {
   'Healthy' : {'Healthy': 0.7, 'Fever': 0.3},
   'Fever' : {'Healthy': 0.4, 'Fever': 0.6}}
emission_probability = {
   'Healthy' : {'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1},
   'Fever' : {'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6}}
\end{verbatim}
\par 这段代码中,起始概率start\_probability表示病人第一次到访时医生认为其所处的HMM状态，他唯一知道的是病人倾向于是健康的。这里用到的特定概率分布不是均衡的，如转移概率大约是{'Healthy': 0.57, 'Fever': 0.43}。转移概率transition\_probability表示潜在的马尔可夫链中健康状态的变化。当天健康的病人仅有30\%的机会第二天会发烧。放射概率emission\_probability表示每天病人感觉的可能性，即$P(normal|Healthy)=0.5$,$P(cold|Healthy)=0.4$,$P(dizzy|Healthy)=0.1$。假如他是健康的，50\%会感觉正常。病人连续三天看医生，医生发现第一天他感觉正常，第二天感觉冷，第三天感觉头晕。于是医生产生了一个问题：怎样的健康状态序列最能够解释这些观察结果，维特比算法可以回答这个问题。
\begin{algorithm}[H]
\caption{维特比算法}
\KwIn{$obs,States,startP,transP,emitP$}
\KwOut{最大概率$prob$,健康状态序列$P[]$}
$V[][] = 0; Path[][]=\emptyset$\;
\For{$t=0;t<len;t++$} {
\uIf{$t==0$}{
\For{$y_i \in States$}{
$V[0][y_i] = startP[y_i] * P(obs[0] | y_i)$ \;
$Path[0][y_i] = \emptyset$\;
}}\Else{
\For{$y_i \in States$}{
$Path[t][y_i] = y_i$\;
$V[t][y_i]=0$\;
\For{$y_j \in States$} {
\If{$V[t][y_i] \le V[t-1][y_j] \times P(y_i|y_j) \times P(obs[t]|y_i) $}{
$V[t][y_i]=V[t-1][y_j] \times P(y_i|y_j)\times P(obs[t]|y_i) $\;
$Path[t][y_i] = y_j$\;}
}}}}
$prob = max(V[len-1][0],V[len-1][1],\cdots)$\;
$y = y_i$，其中$V[len-1][y_i]$为$V[len-1][*]$中最大值\;
定义堆栈$stack = \emptyset$,$stack.push(y)$\;
\For{$t=len-1;t > 0; t--$} {
$y=Path[t][y]$\;
$stack.push(y)$\;
}
\Repeat{$stack.empty()$}{
$P.add(stack.pop())$\;
}
\Return{$prob,P[]$}
\end{algorithm}
\par viterbi算法中，obs为观察结果序列，如['normal','cold','dizzy']，states为一组隐含状态，start\_p 为起始状态概率，trans\_p为转移概率，而emit\_p为放射概率。维特比算法揭示了观察结果['normal','cold','dizzy']最有可能由状态序列['Healthy','Healthy','Fever']产生。换句话说，对于观察到的活动,病人第一天感到正常，第二天感到冷时都是健康的，而第三天发烧了。在实现维特比算法时需注意许多编程语言使用浮点数计算，当p很小时可能会导致结果下溢。避免这一问题的常用技巧是在整个计算过程中使用对数概率，在对数系统中也使用了同样的技巧。当算法结束时，可以通过适当的幂运算获得精确结果。
\subsection{条件随机场}
\par 与最大熵模型相似，条件随机场（Conditional random fields，CRFs）是一种机器学习模型，在自然语言处理的许多领域（如词性标注、中文分词、命名实体识别等）都有比较好的应用效果。条件随机场最早由John D. Lafferty提出，其也是Brown90的作者之一，和贾里尼克相似，在离开IBM后他去了卡耐基梅隆大学继续搞学术研究，2001年以第一作者的身份发表了CRF的经典论文 “Conditional random fields: Probabilistic models for segmenting and labeling sequence data”。
\par 关于条件随机场的参考文献及其他资料，Hanna Wallach在05年整理和维护的这个页面(http://www.inference.phy.cam.ac.uk/hmw26/crf/)非常不错，其中涵盖了自01年CRF提出以来的很多经典论文（不过似乎只到05年，之后并未更新）以及几个相关的工具包(不过也没有包括CRF++），但是仍然非常值得入门条件随机场的读者参考。
\par 标记问题解决分词：就是将词语开始和结束的字标记出来，就能对一个句子完成分词，假设使用两个标记B (开始)，E(结束)对句子进行处理，如：“民主是普世价值”，民/B 主/E 是/B 普/B 世/E 价/B 值/E, 这样标记明确，分词结果就明确了。下面的问题是如何找到最好的标记结果：知道如何用标记的方式解决分词，那么怎么为一个句子找到一个最好的标记序列呢，CRF为这样的问题提供了一个解决方案，对于输入序列$X_1,X_2,\cdots,X_n$($X_i$代表一个字)，求这个输入序列下，某个标记序列$(Y_1,Y_2,\cdots,Y_n)$的概率最大值（每个$X_i$有一个相应的标记$Y_i$）。
\par 这里用一个例子来说明，公式为：$P(y|x,\lambda)=\sum _j \lambda_jF_j(y,x)/Z(x)$。假设使用4种标记，B代表开始，O代表单独成词，M代表词语中间的字，E代表词的结束。一元特征：其中$V_{-1}$表示当前字的前一个字，$V_0$表示当前字，$V_1$表示当前字的后一个字。二元特征：各标记间的转移特征。句子如下：
{\color{red}
\begin{tabbing}
%%如下一行称为样本行，样本行末尾不能以\\结尾，必须以\kill结束，这行仅用来设置制表位，不被显示
民\hspace*{20bp} \=民\hspace*{20bp} \=民\hspace*{20bp} \=民\hspace*{20bp} \=民\hspace*{20bp} \=民\hspace*{20bp} \=民\hspace*{20bp} \=民\kill
%%tabbing环境不会自动分行，换行必须以\\实现
民 \>主 \>是 \>普 \>世 \>价 \>值\\
B \>B \>B \>B \>B \>B \>B\\
O  \>O   \>O   \>O   \>O   \>O   \>O\\
M  \>M   \>M   \>M   \>M   \>M   \>M\\
E   \>E   \>E   \>E   \>E   \>E   \>E\\
\end{tabbing}}
\par Viterbe算法就是在以上由标记组成的图中搜索一条最优路径。对于每一列的每一个标记，我们都要计算到达该标记的分数，这个分数由三部分组成，它本身的一元特征权重W，它前面一个字标记的路径分数PreScore，前面一个字标记到当前标记转移特征权重TransW。
\begin{enumerate}[(1)]
\item 计算第一列的分数score,对‘民’来说，要算它作为B,O,M,E的分数，因为是第一列，所以PreSocre和TransW都是0，只需要计算在一元特征下的权重。对于标记\textbf{B}，它的Score为$S_{1B}$=$W_{1B}$=w(null,民,B)+w(民,B)+w(民,B,主)。意思是：\textbf{w(null,民,B)}，当前字为‘民’标记为B，前面一个字为空；\textbf{w(民,B)}：当前字为‘民’，标记为B；\textbf{w(民,B,主)}：当前字为’民’，标记为B，当前字的后一个字为‘主’。特征权重都是在训练时得到的，对于标记O,M,E，一样要计算$W_{1O}$（标记为O的权重），$W_{1M}$（标记为M的权重），$W_{1E}$（标记为E的权重）,从而得到分数$S_{1O}$,$S_{1M}$,$S_{1E}$。
\item 对于第二列，首先要计算每个标记的一元权重$W_{2B}$，$W_{2O}$,$W_{2M}$,$W_{2E}$。对于B，到达该标记的最大分数为：$$S_{2B}=Max\{(S_{1B}+V_{BB}),(S_{1O}+V_{OB}),(S_{1M}+V_{MB}),(S_{1E}+V_{EB})\}+W_{2B}$$
其中$V_{BB}$为标记B到标记B的转移特征的权重，这个也由训练得到。同样对于第二列的O,M,E也要计算$S_{2O}$，$S_{2M}$，$S_{2E}$。
\item 一直计算到最后一列，从而获得每个字的所有标记分数，得到$S_{7B}$，$S_{7O}$，$S_{7M}$，$S_{7E}$。比较这四个值中的最大值，即为最优路径的分数，然后以该值的标记点为起始点，回溯得到最优路径（因此在计算过程中，要记录到达该标记的前一个标记，用于回溯）。
\end{enumerate}
\subsection{ICTCLAS}
\subsubsection{汉语词性}
\begin{enumerate}[(1)]
\item a 形容词，取英语形容词adjective的第1个字母（共34439个）。
\item ad 直接作状语的形容词，形容词代码a和副词代码d并在一起。（5899）
\item ag 形容词性语素，形容词代码为a，语素代码ｇ前面置以A。（311）
\item an 名形词，具有名词功能的形容词。形容词代码a和名词代码n并在一起。（2838）
\item b 区别词，取汉字“别”的声母。（8734）
\item c 连词，取英语连词conjunction的第1个字母。（25473）
\item d：副词，共47714个，取adverb的第2个字母，因其第1个字母已用于形容词。dg：副词性语素，副词代码为d，语素代码g前面置以d。
\item e 感叹词，取英语exclamation的第1个字母。
\item f 方位词，取汉字“方”的首字母（17248）。
\item g 语素，绝大多数语素都能作为合成词的“词根”，取汉字“根”的声母，不太清楚词根是什么意思。default.dic中有gg，gi等类型，如“计算机管理”，“环境保护”，“超级服务器”等。
\item h：前接成分，取英语head的第1个字母。（48个）;k：后接成分（类似于h，共958个）。
\item i，成语，取英语成语idiom的首字母（5001个）。
\item j，简略字，取汉字“简”的声母（简称，比如“驾照”，“省纪委”，共10293个），但某些词，比如“中纪委”却不具备这个词性。
\item l 习用语，习用语尚未成为成语，有点“临时性”，取“临”的声母。
\item m，数字词，取numerical的第3个字母，nu已有他用（共41036个）。
\item n 名词，取英语名词noun的第1个字母（共237124个）。
\item ng 名词性语素。名词代码为n，语素代码g前面置以n（共4497个）。
\item nr 人名，名词代码n和“人(ren)”的声母并在一起。（20061）
\item ns 地名，名词代码n和处所词代码s(space)并在一起。（27777）
\item nt 机构团体，“团”的声母为t，名词代码n和t并在一起。（3565）
\item nz 专有名词，“专”的首字母为z，名词代码n和z并在一起。（3728）
\item o 拟声词，取英语拟声词onomatopoeia的第1个字母。
\item p 介词，取英语介词prepositional的第1个字母。（39906）
\item q 量词，取英语quantity的第1个字母（24236个，如“倍”，“个”，“斗”）。
\item r 代词，代名词pronoun的第2个字母，因p已用于介词（32367个）。
\item s 处所词，取英语space的第1个字母。
\item t：时间词，共20646个，取英语time的第1个字母。tg：时语素，时间词性语素。时间词代码为t,在语素的代码g前面置以t。
\item u 助词，取英语助词auxiliary，共64711个。
\item v 动词取英语动词verb的第一个字母。
\item vd：副动词，直接作状语的动词。动词和副词的代码并在一起。
\item vg：动词性语素。动词代码为v。在语素的代码g前面置以v。
\item vn 动名词，如“服务”，“批准”，“申请”等，指具有名词功能的动词，词性识别中我偏向于将其设置为名词。
\item w 标点符号（共173046个）。
\item x 非语素字，非语素字只是一个符号，字母$x$通常用于代表未知数、符号。
\item y 语气词，取汉字“语”的声母。
\item z 状态词，取汉字“状”的声母的前一个字母。
\end{enumerate}
\subsubsection{搜狗scel格式}
\par 搜狗的scel词库保存了文本的unicode编码，每两个字节一个字符（中文汉字或英文字母），找出其每部分的偏移即可。主要分为两部分:全局拼音表，由一个个拼音组成，例如台湾的拼音taiwan，tai和wan各自代表一个拼音，字典序格式为\{(index,len,pinyin)\}的列表。汉语词组表，格式为(same,tableLen,py\_table,\{wordLen,word,extLen,ext\})的一个列表。
\begin{enumerate}[(1)]
\item index:两个字节的整数，代表这个拼音在全局拼音表的索引。
\item len:两个字节的整数，拼音的字节长度，如《tai》的长度为6。
\item pinyin:当前拼音，每个字符两个字节，总长度为len，如[tai,a,ai,wan]。
\item same:两字节整数，同音词的个数。tableLen:两字节整数，py\_table的长度。py\_table:整数列表，每个整数两字节，每个整数代表一个拼音的索引，如台湾的拼音为(tai,wan)，存储了两个整数索引，分别指向tai和wan。
\item {wordLen,word,extLen,ext}一共重复same次，wordLen:两字节整数，代表中文词组字节数长度。word:中文词组，每个中文汉字两个字节，数组长度为wordLen。extLen:两个字节整数，代表扩展信息长度，貌似都是10。ext:扩展信息，前两个字节是一个整数（多半是词频），后八个字节全是0。
\end{enumerate}
\subsection{关键词提取}
\par TF-IDF（term frequency,inverse document frequency）是一种用于信息检索与文本挖掘的常用加权技术。TF-IDF是一种统计方法，用以评估某个词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了TF-IDF以外，互联网上的搜寻引擎还会使用基于链接分析的评级方法(page rank算法)，以确定文件在搜寻结果中出现的顺序。
\par TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。TF表示词条在文档d中出现的频率。IDF的思想是：如果包含词条t的文档越少，也就是n越小，IDF越大，则说明词条t具有很好的类别区分能力。如果某一类文档C中包含词条t的文档数为m，而其它类包含t的文档总数为k，显然所有包含t的文档数n=m+k，当m大的时候，n也大，按照IDF公式得到的IDF的值会小，就说明该词条t类别区分能力不强。
\par 如果一个词条在一个类的文档中频繁出现，则说明该词条能够很好代表这个类的文本的特征，这样的词条应该给它们赋予较高的权重，并选来作为该类文本的特征词以区别于其它类文档。这就是IDF的不足之处，在一份给定的文件里，词频（term frequency，TF）指的是某一个给定的词语在该文件中出现的频率。这个数字是对词数(term count)的归一化，以防止它偏向长的文件。（同一个词语在长文件里可能会比短文件有更高的词数，而不管该词语重要与否。）对于在某一特定文件里的词语 $t_i$来说，它的重要性可表示为：
$$tf_{i,j} = \frac{n_{i,j}}{\sum_k n_{k,j}}$$
\par 以上式子中$n_{i,j}$是该词在文件$d_j$中的出现次数，而分母则是文件$d_j$中所有词($\forall t_k$)的出现次数之和。逆向文件频率（inverse document frequency，IDF）是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到：
$$idf_i = \log\frac{|D|}{|\{j: t_{i} \in d_{j}\}|}$$
\par 其中$|D|$：语料库中的文件总数，$|\{j: t_{i} \in d_{j}\}|$：包含词语$t_i$的文件数目（即$n_{i,j} \neq 0$的文件数目）如果该词语不在语料库中，就会导致被除数为零，因此一般情况下使用$1 + |\{j : t_i \in d_j\}|$，然后$tfidf_{i,j} = tf_{i,j} \times idf_{i}$。某一特定文件内的高频率词语，以及该词语在整个文件集合中的低文档频率，可产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。
\par 一：有很多不同的数学公式可以用来计算TF-IDF，词频 (TF) 是一词语出现的次数除以该文件的总词语数。假如一篇文件的总词语数是100个，而词语“母牛”出现了3次，那么“母牛”一词在该文件中的词频就是3/100=0.03。一个计算文件频率 (DF) 的方法是测定有多少份文件出现过“母牛”一词，然后除以文件集里包含的文件总数。所以，如果“母牛”一词在1,000份文件出现过，而文件总数是10,000,000份的话，其逆向文件频率就是 log(10,000,000 / 1,000)=4。最后的TF-IDF的分数为0.03*4=0.12。
\par 二：根据关键字k1,k2,k3进行搜索结果的相关性就变成TF1*IDF1 + TF2*IDF2 + TF3*IDF3。比如document1的term总量为1000，k1,k2,k3在document1出现的次数是100，200，50。包含了 k1, k2, k3的document总量分别是 1000， 10000，5000。document set的总量为10000。 
\begin{eqnarray*}
TF_1 = \frac{100}{1000} = 0.1 & IDF_1 = log\frac{10000}{1000} = log(10) = 2.3 \\
TF_2 = \frac{200}{1000} = 0.2 & IDF_2 = log\frac{10000}{10000} = log(1) = 0; \\
TF_3 = \frac{50}{1000} = 0.05 & IDF_3 = log\frac{10000}{5000} = log(2) = 0.69
\end{eqnarray*}
\par 这样关键字k1,k2,k3与document1的相关性$R=0.1\times 2.3 + 0.2\times 0 + 0.05\times 0.69 = 0.2645$，其中k1比k3的比重在document1要大，k2的比重是0。
\par 三：在某个一共有一千词的网页中“原子能”、“的”和“应用”分别出现了 2 次、35 次 和 5 次，那么它们的词频就分别是 0.002、0.035 和 0.005。 我们将这三个数相加，其和 0.042 就是相应网页和查询“原子能的应用” 相关性的一个简单的度量。概括地讲，如果一个查询包含关键词 w1,w2,...,wN, 它们在一篇特定网页中的词频分别是:TF1, TF2, ..., TFN。 （TF: term frequency)。 那么，这个查询和该网页的相关性就是:TF1 + TF2 + ... + TFN。
\par 在上面的例子中，词“的”词频很大，而它对确定网页的主题几乎没用。称这种词叫“停用词”（Stopwords)，也就是说在度量相关性时不应考虑它们的频率。在汉语中，停用词还有“是”、“和”、“中”、“地”、“得”等几十个。忽略这些停用词后，上述网页的相似度就变成了0.007，其中“原子能”贡献了 0.002，“应用”贡献了 0.005。细心的读者可能还会发现另一个小的漏洞。在汉语中，“应用”是个很通用的词，而“原子能”是个很专业的词，后者在相关性排名中比前者重要。因此需要给汉语中的每一个词一个权重，这个权重的设定必须满足下面两个条件：
\begin{verbatim}
1.一个词预测主题能力越强，权重就越大，反之，权重就越小。
2.应删除词的权重应该是零。
\end{verbatim}
\par 在网页中看到“原子能”这个词，或多或少地能了解网页的主题。看到“应用”一词，对主题基本上还是一无所知。因此，“原子能“的权重应该比应用大。如果一个关键词只在很少的网页中出现，通过它就容易锁定搜索目标，它的权重也就应该大。反之如果一个词在大量网页中出现，看到它仍然不清楚要找什么内容，因此它应该小。概括地讲，假定一个关键词$w$在$D$个网页中出现过，那么$D$越大，$w$的权重越小，反之亦然。信息检索中，使用最多的权重是IDF。假定中文网页数是10亿，停用词“的”在所有的网页中都出现，那么它的IDF为零。假如专用词“原子能”在两百万个网页中出现，则它的权重IDF为6.2。又假定通用词“应用”，出现在五亿个网页中，它的权重IDF只有0.7。也就只说，在网页中找到一个“原子能”的比配相当于找到九个“应用”的匹配。利用IDF，上述相关性计算公式就由词频的简单求和变成了加权求和，即$tf_1\times idf_1+tf_2\times idf_2+ \cdots+tf_n\times idf_n$。上例中，该网页和“原子能的应用”的相关性为0.0161，其中“原子能”贡献了0.0126，而“应用”只贡献了0.0035。
\par TF-IDF权重计算方法经常会和余弦相似性（cosine similarity）一同使用于向量空间模型中，用以判断两份文件之间的相似性。
\par TF-IDF是建立在这样一个假设之上的：对区别文档最有意义的词语应该是那些在文档中出现频率高，而在整个文档集合的其他文档中出现频率少的词语，所以如果特征空间取TF词频作为测度，就可以体现同类文本的特点。另外考虑到单词区别不同类别的能力，TF-IDF认为一个单词出现的文本频数越小，它区别不同类别文本的能力就越大。因此引入了逆文本频度IDF的概念，以TF和IDF的乘积作为特征空间的取值测度，并用它完成对权值TF的调整，调整权值的目的在于突出重要单词，抑制次要单词。但是在本质上IDF是一种试图抑制噪声的加权，并且单纯地认为文本频率小的单词就越重要，文本频率大的单词就越无用，显然这并不完全正确。IDF的简单结构并不能有效地反映单词的重要程度和特征词的分布情况，使其无法很好地完成对权值调整的功能。
\par 在TF-IDF中并没有体现出单词的位置信息，对web文档，权重的计算方法应该体现出HTML的结构特征。特征词在不同的标记符中对文章内容的反映程度不同，其权重的计算方法也应不同。因此应该对于处于网页不同位置的特征词赋予不同的系数，然后乘以特征词的词频，以提高文本表示的效果。



