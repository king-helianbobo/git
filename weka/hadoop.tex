\section{关于MapReduce}
\par 下载\textsl{hadoop-1.2.1}包，然后使用\textsl{ant eclipse}命令，生成eclipse工程文件，在ant编译时，可能出现错误：\textsl{libtoolize: command not found},这是因为没有安装libtool包，使用\textsf{sudo apt-get install libtool}安装它们，这样，应该能生成工程文件。
\subsection{InputFormat类}
\par InputFormat类是Hadoop Map Reduce框架中的基础类之一。该类主要用来定义两件事情：
\begin{enumerate}[(1)]
\item 数据分割(Data splits)，生成列表List<InputSplit>;
\item 记录读取器(Record reader)，负责给执行任务的MapTask提供一个个Key/Value对。
\end{enumerate}
\par WordCount中的TextInputFormat(TextInputFormat --> FileInputFormat --> InputFormat， --> 标识继承关系)，其核心功能是提供了getSplits函数（即数据分割部分，实现过程在FileInputFormat类中）和LineRecordReader类。
\par 数据分割（InputSplit）是MapReduce框架中的基础概念之一，它定义了单个MapTask的输入数据源（MapTask可以没有输入数据源）的大小及其所处的DataNode的位置信息。WordCount例子中，仍然是按照块大小（默认情况下，64M）将文本文件分成以64M字节为单位的一个个InputSplit，每个InputSplit将作为分配任务的基本单元，即每个MapTask的数据输入源。
\par RecordReader主要负责从输入的hdfs文件（逻辑split，也就是每个MapTask执行的输入数据源）上读取数据并将它们以键值对的形式提交给MapTask（一次Map任务的执行过程称为MapTask，封装于独立的Java Virtual Machine中），RecordReader提供抽象的nextKeyValue方法，用于判断输入InputSplit中是否还有待处理的Key/Value对。
\par WordCount示例中，RecordReader的具体实现类为LineRecordReader，对任意一个InputSplit，它读取的Start位置并不是该DataBlock的开头，而是跳转到第一行单词的末尾，以第一行的末尾作为读取的Start位置。因此，LineRecordReader，在很大可能性上不只读取当前的DataBlock，它还会读取InputSplit的下个DataBlock，直到获得一个完整的字符串行（单词），这样就能避免一行单词被两个InputSplit拆分。
\subsection{OutputFormat类}
OutputFormat类是Hadoop Map Reduce框架中的基础类之一。该类主要用来定义两件事情：
\begin{enumerate}[(1)]
\item 提供RecordWriter，MapReduce作业的输出均由RecordWriter负责，它负责写入一个个Key/Value对，至于写到hdfs还是本地文件系统，或者写入一个还是多个文件，取决于RecordWriter的实现。
\item 提供OutputCommitter，OutputCommitter负责：作业的初始化（在MapReduce中称为JobSetupTask），作业失败时的清除（JobCleanupTask），任务失败时的清除（TaskCleanupTask）和作业提交（commitJob），它的默认实现是FileOutputCommitter。
\end{enumerate}
\par OutputCommiter执行的任务包括：
\begin{enumerate}[(1)]
\item JobSetupTask,作业开始时，在hdfs上建立作业临时目录，调用OutputCommitter的setupJob接口。
\item TaskCleanupTask,构成作业的某个Task失败时，调用OutputCommitter的abortTask接口。
\item JobCleanupTask,作业结束时，如果作业失败，调用OutputCommitter的abortJob接口，如果作业成功，调用OutputCommitter的commitJob接口。
\end{enumerate}
\par MapTask和ReduceTask都可能会调用RecordWriter，这个输出会写到哪里去呢？如果不存在Reduce任务，那么Map任务的输出当然是由RecordWriter负责写入了。查阅MapTask类的源码，有如下代码：
\begin{verbatim}
if (job.getNumReduceTasks() == 0) {
  output = new NewDirectOutputCollector;
} else {
  output = new NewOutputCollector;
}
\end{verbatim}
\par 也就是说，如果没有ReduceTask，则RecordWriter对MapTask负责，反之，RecordWriter对ReduceTask负责，NewOutputCollector将负责MapTask的输出，它调用MapOutputBuffer来输出其中间结果，这种情况下，中间结果将写入本地文件系统，避免了写入HDFS带来的较大开销，但这种情况下，中间结果对用户是不可见的。
\subsection{Mapreduce中的两个Mapper}
\par MapReduce中存在两种API，旧的API来自于包org.apache.hadoop.mapred，新的API来自于包org.apache.hadoop.mapreduce。新的API有Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT>类，调用者通过继承Mapper类，并重载void map(KEYIN key, VALUEIN value, Context context) 函数，以执行map任务。
\par Context类继承了MapContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT>,是Mapper中核心类，给MapTask,ReduceTask提供包括：
\begin{itemize}
\item InputFormat，包含了RecordReader<KEYIN,VALUEIN>，InputSplit等信息，其中reader负责给Mapper提供K/V对,InputSplit定义了MapTask的输入数据源;
\item OutputFormat，包含了OutputCommitter，RecordWriter<KEYOUT,VALUEOUT>等信息，writer负责写入Mapper或Reducer的计算结果;
\item StatusReporter,JobID，TaskAttemptID等信息，还包括OutputCommitter，committer负责Task的一些清理工作，可以更多地与MapReduce执行交互;
\item 新API调用\textbf{inputFormat.createRecordReader(split,taskContext);}旧API调用\textbf{job.getInputFormat().getRecordReader(split,job,reporter)}函数。
\end{itemize}
\subsection{Hadoop中配置log4j}
开发MapReduce作业时，使用System.out.print()方式和将日志输出到本地文件中比较麻烦，因此使用log4j库打印日志。但使用Hadoop库时，自己写的log4j.xml(或log4j.properties)文件会被MapReduce框架覆盖掉，从而不起作用，此时只能使用Hadoop提供的log4j.properties文件。在自己写的类的main()函数中，可以显式地加载log4j.xml或log4j.properties文件，代码为：\textbf{DOMConfigurator.configure("src/log4j.xml");}或者\textbf{PropertyConfigurator.configure("src/log4j.properties");}
\par 修改\$\{HADOOP\_HOME\}/etc/hadoop/目录下的log4j.properties文件，在文件末尾添加如下几句：
\begin{verbatim}
log4j.logger.org.jpgExtractor=INFO,jpgExtractor
log4j.appender.jpgExtractor=org.apache.log4j.FileAppender
log4j.appender.jpgExtractor.File=/tmp/log4j.log
log4j.appender.jpgExtractor.layout=org.apache.log4j.PatternLayout
log4j.appender.jpgExtractor.layout.ConversionPattern=%d %-5p [%c{1}] %m%n
log4j.additivity.org.jpgExtractor=false
# log4j.appender.jpgExtractor.filter=org.apache.log4j.varia.LevelRangeFilter
# log4j.appender.jpgExtractor.filter.LevelMin=DEBUG
# log4j.appender.jpgExtractor.filter.LevelMax=ERROR
\end{verbatim}
\par 这表示将package(org.jpgExtractor)下的Log输出到名为jpgExtractor的LogAppender中。Namenode（JobTracker）和Datanode（TaskTracker）下的log4j.properties文件都必须按如上所说进行修改，修改完后重启整个Hadoop。
\par 当一个作业执行时，由Namenode（JobTracker）执行的那部分代码（如InputFormat中的getSplits函数和isSplitable函数），其输出结果保存在"/tmp/log4j.log"文件中，而由Datanode（TaskTracker）执行的那部分代码，其输出结果没有放在"/tmp/log4j.log"文件里，而是输出到了\$\{HADOOP\_HOME\}/logs/userlogs/\$\{jobId\}/\$\{TaskAttemptId\}/stdlog文件中。
\par 修改了的配置文件只对JobTracker，TaskTracker等MapReduce框架中的代码有效，而对执行MapTask，ReduceTask的Java虚拟机无效，因为当一个Map或Reduce任务执行时，MapReduce框架重新给运行任务的JVM设置了参数，这些参数不再调用Hadoop配置目录下的log4j.properties了。
\par 执行任务的JVM参数被重新设置，至于Hadoop是怎么设置的，查阅Hadoop源码中的log4j.properties配置文件，发现TLA这个\textsl{TaskLogAppender}，其描述如下：
\begin{verbatim}
log4j.appender.TLA=org.apache.hadoop.mapred.TaskLogAppender
log4j.appender.TLA.taskId=${hadoop.tasklog.taskid}
log4j.appender.TLA.isCleanup=${hadoop.tasklog.iscleanup}
log4j.appender.TLA.totalLogFileSize=${hadoop.tasklog.totalLogFileSize}
log4j.appender.TLA.layout=org.apache.log4j.PatternLayout
log4j.appender.TLA.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
\end{verbatim}
然后通过\textbf{find ./ -iname '*.java' |xargs grep -n 'TLA'}命令，找到如下代码：\textbf{vargs.add("-Dhadoop.root.logger=INFO,TLA");}
\par MapReduce框架在开启执行MapTask或ReduceTask的JVM时，修改了\textsl{hadoop.root.logger}，从而将rootLogger的输出重定向到syslog文件中。为了实现在Mapper和Reducer中输出Log信息到本地文件系统或其它地方时，继承Mapper或Reducer的setup(context)函数，在其中添加如下代码：
\begin{verbatim}
protected void setup(Context context) 
      throws IOException,InterruptedException{
  FileAppender  fa = new FileAppender();
  fa.setName("FileLogger");
  fa.setFile("/tmp/lau.log");
  fa.setLayout(new PatternLayout("%d %-5p [%c{1}] %m%n"));
  fa.setThreshold(Level.INFO);
  fa.setAppend(true);
  fa.activateOptions();
  Logger.getRootLogger().getLoggerRepository().resetConfiguration();
  Logger.getRootLogger().addAppender(fa);
}
\end{verbatim}
\par 这种方式可以输出用户作业的日志到本地文件("/tmp/lau.log")，但对OutputCommiter的日志输出无效，因为如上代码只能修改Mapper和Reducer子进程的setup函数，而无法修改执行SetupTask和CleanupTask的Java虚拟机的默认设置，下面附上Log4j中ConversionPattern参数的格式意义：
\begin{verbatim}
%c 输出日志信息所属类的全名
%d 输出日志时间点，默认格式为ISO8601，也可以指定格式，比如：%d{yyyy-MM-dd HH:mm:ss}，输出类似：2013-10-18 22:10:28
%l 输出日志事件的发生位置，包括类名、发生的线程，以及在代码中的行数，形如org.ansj.library.UserDefineLibrary.loadFile(UserDefineLibrary.java:165)。
%L 输出日志事件在代码中的行数。
%m 输出代码中指定的信息，如log.info(message)中的message;
%n 输出一个回车换行符，Windows平台为'\r\n'，Unix平台为'\n';
%p 输出优先级，即DEBUG，INFO，WARN，ERROR，FATAL。如果是调用debug()输出的，则为DEBUG，依此类推
%r 输出自应用启动到输出该日志信息所耗费的毫秒数
%t 输出产生该日志事件的线程名
\end{verbatim}
\subsection{Job的提交过程}
在org.apache.hadoop.mapreduce.Job类中，submit函数将作业提交给MapReduce框架执行，其由waitForCompletion函数调用。
Submit函数 --> setUseNewAPI(设置使用新的API函数) --> 生成JobClient对象（JobClient创建与JobTracker的RPC协议）
现在需要搞清楚的是Job的提交过程，
\par 当在hadoop集群上运行作业时，不能用Ctrl+C来杀掉作业，可行的办法是执行：hadoop job -list，查看Hadoop集群上有哪些作业，然后调用：hadoop job -kill 'jobId'，来杀掉标识符为jobId的作业。
\subsection{Java客户端访问hdfs}
\par Java客户端指的是，在非hadoop集群上的机器访问hdfs。
\begin{enumerate}[(1)]
\item HDSF添加用户并设置权限，Hadoop为每个用户设置一定的权限,当使用Java Client端访问hdfs时,实际上是以某个用户来访问hdfs，hdfs会检查该用户是否具有相应权限，比如hadoop的root用户具有对全部文件的读写权限。在hdfs上建立用户,并赋予用户权限的过程如下:
\begin{verbatim}
hadoop fs -mkdir /user (如果没有/user目录)
hadoop fs -mkdir /user/liubo(建立liubo目录)
hadoop fs -chown liubo:liubo /user/liubo
\end{verbatim}
\par 默认情况下的读写权限为700,也就是说,只有user具备读写权限(hdfs无可执行文件,因此没有必要设置执行权限，700实际上是600)
\item Java客户端访问hdfs时，读取用户名过程为：先读取HADOOP\_USER\_NAME系统环境变量，如果为null，则读取java环境变量中的HADOOP\_USER\_NAME，如果仍然为null，则以系统当前的user为默认的用户名。比如在Ubuntu 12.04下的的用户名是lau,那么,当没有设置系统环境变量和java环境变量时，Java Client会以lau这个用户名访问hdfs，假如lau这个用户没有权限，则执行会出错。
\item 设置系统环境变量比较麻烦，更改系统当前登录用户更加麻烦，最容易的方式是设置Java环境变量，设置代码为：
\begin{verbatim}
Properties property = System.getProperties();
property.setProperty("HADOOP_USER_NAME", "root");
\end{verbatim}
\par 以这种方式，就可以读写hdfs目录下的任何文件了。从这里也能看出，hdfs的安全机制十分薄弱，必须依赖于宿主操作系统的安全机制。
\end{enumerate}