<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!-- Put site-specific property overrides in this file. -->
<configuration>
  <property>
    <name>plugin.includes</name>
    <value>protocol-(http|httpclient)|urlfilter-regex|parse-(html|js|tika)|index-(basic|anchor)|indexer-elastic|scoring-opic|urlnormalizer-(pass|regex|basic)</value>
    <description>Regular expression naming plugin directory names to
    include.  Any plugin not matching this expression is excluded.
    In any case you need at least include the nutch-extension points plugin. By
    default Nutch includes crawling just HTML and plain text via HTTP,
    and basic indexing and search plugins. In order to use HTTPS please enable
    protocol-httpclient, but be aware of possible intermittent problems with the
    underlying commons-httpclient library.
    </description>
  </property>

  <!-- Elasticsearch properties -->
  <property>
    <name>elastic.host</name>
    <value>localhost</value>
    <description>The hostname to send documents to using TransportClient. Either host
    and port must be defined or cluster.</description>
  </property>
  
  <property>
    <name>elastic.port</name>
    <value>9300</value>
    <description>
    </description>
  </property>
  
  <property>
    <name>elastic.cluster</name>
    <value>elasticsearch</value>
    <description>The cluster name to discover. Either host and potr must be defined
    or cluster.</description>
  </property>
  
  <property>
    <name>elastic.index</name>
    <value>nutch</value>
    <description>Default index to send documents to.</description>
  </property>
  
  <property>
    <name>elastic.max.bulk.docs</name>
    <value>250</value>
    <description>Maximum size of the bulk in number of documents.</description>
  </property>
  
  <property>
    <name>elastic.max.bulk.size</name>
    <value>2500500</value>
    <description>Maximum size of the bulk in bytes.</description>
  </property>
  <!-- Elasticsearch properties -->

  <!-- http client properties -->
  <property>
    <name>http.agent.name</name>
    <value>soul_seg</value>
    <description>HTTP 'User-Agent' request header. MUST NOT be empty - 
    please set this to a single word uniquely related to your organization.
    NOTE: You should also check other related properties:
    http.robots.agents
    http.agent.description
    http.agent.url
    http.agent.email
    http.agent.version
    and set their values appropriately.
    </description>
  </property>

  <property>
    <name>http.robots.agents</name>
    <value>soul_seg,*</value>
    <description>The agent strings we'll look for in robots.txt files,
    comma-separated, in decreasing order of precedence. You should
    put the value of http.agent.name as the first agent name, and keep the
    default * at the end of the list. E.g.: BlurflDev,Blurfl,*
    </description>
  </property>

  <property>
    <name>http.content.limit</name>
    <value>-1</value>
    <description>The length limit for downloaded content using the http://
    protocol, in bytes. If this value is nonnegative (>=0), content longer
    than it will be truncated; otherwise, no truncation at all. Do not
    confuse this setting with the file.content.limit setting.
    </description>
  </property>

  <property>
    <name>http.verbose</name>
    <value>true</value>
    <description>If true, HTTP will log more verbosely.</description>
  </property>

  <property>
    <name>parser.timeout</name>
    <value>-1</value>
  </property>
</configuration>
